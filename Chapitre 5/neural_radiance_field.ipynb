{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1701780451265,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "3-cuSDF__gHi"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxtPMcM1_gHn"
   },
   "source": [
    "# Ajuster un simple champ de radiance neuronale via raymarching\n",
    "\n",
    "\n",
    "Ce didacticiel montre comment ajuster le champ de radiance neuronale à un ensemble de vues d'une scène à l'aide du rendu de fonction implicite différentiable.\n",
    "\n",
    "\n",
    "Plus précisément, ce tutoriel expliquera comment :\n",
    "1. Créez un moteur de rendu de fonction implicite différenciable avec un échantillonnage de grille d'image ou de rayons de Monte Carlo.\n",
    "2. Créez un modèle implicite d'une scène.\n",
    "3. Ajustez la fonction implicite (Neural Radiance Field) en fonction des images d'entrée à l'aide du moteur de rendu implicite différentiable.\n",
    "4. Visualisez la fonction implicite apprise.\n",
    "\n",
    "\n",
    "Notez que le modèle implicite présenté est une version simplifiée de NeRF :<br>\n",
    "_Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng : NeRF : Représenter des scènes sous forme de champs de rayonnement neuronal pour la synthèse de vues, ECCV 2020._\n",
    "\n",
    "\n",
    "Les simplifications incluent :\n",
    "* *Échantillonnage de rayons* : Ce carnet n'effectue pas d'échantillonnage de rayons stratifié mais plutôt un échantillonnage de rayons à des profondeurs équidistantes.\n",
    "* *Rendu* : nous effectuons une seule passe de rendu, contrairement à l'implémentation originale qui effectue une passe de rendu grossière et fine.\n",
    "* *Architecture* : Notre réseau est moins profond, ce qui permet une optimisation plus rapide, éventuellement au détriment des détails de surface.\n",
    "* *Perte de masque* : Puisque nos observations incluent des masques de segmentation, nous optimisons également une perte de silhouette qui oblige les rayons soit à être entièrement absorbés à l'intérieur du volume, soit à le traverser complètement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4siOCgIj_gHs"
   },
   "source": [
    "## 0. Install and Import modules\n",
    "Ensure `torch` and `torchvision` are installed. If `pytorch3d` is not installed, install it using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18428,
     "status": "ok",
     "timestamp": 1701780469690,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "nI4u0JVi_gHt",
    "outputId": "b987231a-766b-480b-fa17-75bf308ca895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fvcore\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting iopath\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n",
      "Collecting yacs>=0.1.6 (from fvcore)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.3.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.5.0)\n",
      "Collecting portalocker (from iopath)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: fvcore, iopath\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=5ca404a633fceb3e2e62cb4ff463f7b933b6d3a124718f6ffa86df6eb8edccca\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=e1262871c300db8baff8c61f64b82ae27b9f9057411b7490b33b7204eb55bb33\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "Successfully built fvcore iopath\n",
      "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
      "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n",
      "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu118_pyt210/download.html\n",
      "Collecting pytorch3d\n",
      "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu118_pyt210/pytorch3d-0.7.5-cp310-cp310-linux_x86_64.whl (20.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m165.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from pytorch3d) (0.1.10)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (1.23.5)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (4.66.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (2.3.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (9.4.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorch3d) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (4.5.0)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->pytorch3d) (2.8.2)\n",
      "Installing collected packages: pytorch3d\n",
      "Successfully installed pytorch3d-0.7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "need_pytorch3d=False\n",
    "try:\n",
    "    import pytorch3d\n",
    "except ModuleNotFoundError:\n",
    "    need_pytorch3d=True\n",
    "if need_pytorch3d:\n",
    "    if torch.__version__.startswith(\"2.1.\") and sys.platform.startswith(\"linux\"):\n",
    "        # We try to install PyTorch3D via a released wheel.\n",
    "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "        version_str=\"\".join([\n",
    "            f\"py3{sys.version_info.minor}_cu\",\n",
    "            torch.version.cuda.replace(\".\",\"\"),\n",
    "            f\"_pyt{pyt_version_str}\"\n",
    "        ])\n",
    "        !pip install fvcore iopath\n",
    "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "    else:\n",
    "        # We try to install PyTorch3D from source.\n",
    "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1701780469691,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "_BbCoQ4u_gHu"
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %matplotlib notebook\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.transforms import so3_exp_map\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras,\n",
    "    NDCMultinomialRaysampler,\n",
    "    MonteCarloRaysampler,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    ImplicitRenderer,\n",
    "    RayBundle,\n",
    "    ray_bundle_to_ray_points,\n",
    ")\n",
    "\n",
    "# obtain the utilized device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    print(\n",
    "        'Please note that NeRF is a resource-demanding method.'\n",
    "        + ' Running this notebook on CPU will be extremely slow.'\n",
    "        + ' We recommend running the example on a GPU'\n",
    "        + ' with at least 10 GB of memory.'\n",
    "    )\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1701780470294,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "KjuI7sag_gHu",
    "outputId": "267450fb-96a9-42eb-ed57-47a1f02d7d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-05 12:47:49--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1608 (1.6K) [text/plain]\n",
      "Saving to: ‘plot_image_grid.py’\n",
      "\n",
      "plot_image_grid.py  100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-05 12:47:49 (33.2 MB/s) - ‘plot_image_grid.py’ saved [1608/1608]\n",
      "\n",
      "--2023-12-05 12:47:49--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/generate_cow_renders.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6779 (6.6K) [text/plain]\n",
      "Saving to: ‘generate_cow_renders.py’\n",
      "\n",
      "generate_cow_render 100%[===================>]   6.62K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-05 12:47:49 (99.0 MB/s) - ‘generate_cow_renders.py’ saved [6779/6779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
    "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/generate_cow_renders.py\n",
    "from plot_image_grid import image_grid\n",
    "from generate_cow_renders import generate_cow_renders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KUPUX-c_gHv"
   },
   "source": [
    "OU en cas d'exécution locale, décommentez et exécutez la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701780470295,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "kVwuNTEb_gHv"
   },
   "outputs": [],
   "source": [
    "# from utils.generate_cow_renders import generate_cow_renders\n",
    "# from utils import image_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QRNDWw-_gHw"
   },
   "source": [
    "## 1. Générer des images de la scène et des masques\n",
    "\n",
    "La cellule suivante génère nos données d'entraînement.\n",
    "Il restitue le maillage de vache du didacticiel `fit_textured_mesh.ipynb` sous plusieurs points de vue et renvoie :\n",
    "1. Un lot de tenseurs d'image et de silhouette produits par le moteur de rendu de maillage de vache.\n",
    "2. Un ensemble de caméras correspondant à chaque rendu.\n",
    "\n",
    "Remarque : Pour les besoins de ce tutoriel, qui vise à expliquer les détails du rendu implicite, nous n'expliquons pas comment fonctionne le rendu de maillage, implémenté dans la fonction `generate_cow_renders`. Veuillez vous référer à `fit_textured_mesh.ipynb` pour une explication détaillée du rendu du maillage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13034,
     "status": "ok",
     "timestamp": 1701780483326,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "0wprPHdZ_gHw",
    "outputId": "7ebff399-8a5a-4042-a052-24cf968352ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40 images/silhouettes/cameras.\n"
     ]
    }
   ],
   "source": [
    "target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=40, azimuth_range=180)\n",
    "print(f'Generated {len(target_images)} images/silhouettes/cameras.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8zmnMvU_gHw"
   },
   "source": [
    "## 2. Initialisez le moteur de rendu implicite\n",
    "\n",
    "Ce qui suit initialise un moteur de rendu implicite qui émet un rayon à partir de chaque pixel d'une image cible et échantillonne un ensemble de points uniformément espacés le long du rayon. À chaque point de rayon, la densité et la valeur de couleur correspondantes sont obtenues en interrogeant l'emplacement correspondant dans le modèle neuronal de la scène (le modèle est décrit et instancié dans une cellule ultérieure).\n",
    "\n",
    "Le moteur de rendu est composé d'un *raymarcher* et d'un *raysampler*.\n",
    "- Le *raysampler* est chargé d'émettre des rayons à partir des pixels de l'image et d'échantillonner les points le long d'eux. Ici, nous utilisons deux rayamplers différents :\n",
    "     - `MonteCarloRaysampler` est utilisé pour générer des rayons à partir d'un sous-ensemble aléatoire de pixels du plan image. Le sous-échantillonnage aléatoire des pixels est effectué lors de l'**entraînement** pour diminuer la consommation mémoire du modèle implicite.\n",
    "     - `NDCMultinomialRaysampler` qui suit la convention standard de grille de coordonnées PyTorch3D (+X de droite à gauche ; +Y de bas en haut ; +Z loin de l'utilisateur). En combinaison avec le modèle implicite de la scène, `NDCMultinomialRaysampler` consomme une grande quantité de mémoire et, par conséquent, n'est utilisé que pour visualiser les résultats de l'entraînement au moment du **test**.\n",
    "- Le *raymarcher* prend les densités et les couleurs échantillonnées le long de chaque rayon et restitue chaque rayon en une couleur et une valeur d'opacité du pixel source du rayon. Ici, nous utilisons le `EmissionAbsorptionRaymarcher` qui implémente l'algorithme standard de raymarching Emission-Absorption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701780483327,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "STrRmwmc_gHx"
   },
   "outputs": [],
   "source": [
    "# render_size décrit la taille des deux côtés du\n",
    "# images rendues en pixels. Puisqu'un avantage de\n",
    "# Les champs de rayonnement neuronal sont des rendus de haute qualité\n",
    "# avec une quantité importante de détails, nous rendons\n",
    "# la fonction implicite au double de la taille de\n",
    "# images cibles.\n",
    "render_size = target_images.shape[1] * 2\n",
    "\n",
    "# Notre scène rendue est centrée autour de (0,0,0)\n",
    "# et est enfermé dans un cadre de délimitation\n",
    "# dont le côté est à peu près égal à 3,0 (unités mondiales).\n",
    "volume_extent_world = 3.0\n",
    "\n",
    "# 1) Instanciez les rayamplers.\n",
    "\n",
    "# Ici, NDCMultinomialRaysampler génère une image rectangulaire\n",
    "# grille de rayons dont les coordonnées suivent le PyTorch3D\n",
    "# conventions de coordonnées.\n",
    "raysampler_grid = NDCMultinomialRaysampler(\n",
    "    image_height=render_size,\n",
    "    image_width=render_size,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "# MonteCarloRaysampler génère un sous-ensemble aléatoire\n",
    "# de rayons `n_rays_per_image` émis depuis le plan image.\n",
    "raysampler_mc = MonteCarloRaysampler(\n",
    "    min_x = -1.0,\n",
    "    max_x = 1.0,\n",
    "    min_y = -1.0,\n",
    "    max_y = 1.0,\n",
    "    n_rays_per_image=750,\n",
    "    n_pts_per_ray=128,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "# 2) Instanciez le raymarcher.\n",
    "# Ici, nous utilisons le standard EmissionAbsorptionRaymarcher\n",
    "# qui marche le long de chaque rayon afin de rendre\n",
    "# le rayon en un seul vecteur de couleur 3D\n",
    "# et un scalaire d'opacité.\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "\n",
    "# Enfin, instanciez les rendus implicites\n",
    "# pour les deux rayamplers.\n",
    "renderer_grid = ImplicitRenderer(\n",
    "    raysampler=raysampler_grid, raymarcher=raymarcher,\n",
    ")\n",
    "renderer_mc = ImplicitRenderer(\n",
    "    raysampler=raysampler_mc, raymarcher=raymarcher,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siP301xy_gHx"
   },
   "source": [
    "## 3. Définir le modèle de champ de radiance neuronale\n",
    "\n",
    "Dans cette cellule, nous définissons le module `NeuralRadianceField`, qui spécifie un champ continu de couleurs et d'opacités sur le domaine 3D de la scène.\n",
    "\n",
    "La fonction `forward` de `NeuralRadianceField` (NeRF) reçoit en entrée un ensemble de tenseurs qui paramétrent un faisceau de rayons de rendu. Le faisceau de rayons est ensuite converti en points de rayons 3D dans les coordonnées mondiales de la scène. Chaque point 3D est ensuite mappé à une représentation harmonique à l'aide du calque `HarmonicEmbedding` (défini dans la cellule suivante). Les intégrations harmoniques entrent ensuite dans les branches _color_ et _opacity_ du modèle NeRF afin d'étiqueter chaque point de rayon avec un vecteur 3D et un scalaire 1D compris entre [0-1] qui définissent respectivement la couleur RVB et l'opacité du point.\n",
    "\n",
    "Étant donné que NeRF a une grande empreinte mémoire, nous implémentons également la méthode « NeuralRadianceField.forward_batched ». La méthode divise les rayons d'entrée en lots et exécute la fonction « forward » pour chaque lot séparément dans une boucle for. Cela nous permet de restituer un grand nombre de rayons sans manquer de mémoire GPU. Standardement, `forward_batched` serait utilisé pour restituer les rayons émis par tous les pixels d'une image afin de produire un rendu en taille réelle d'une scène."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701780483327,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "2KMpr733_gHy"
   },
   "outputs": [],
   "source": [
    "class HarmonicEmbedding(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, omega0=0.1):\n",
    "        \"\"\"\n",
    "         Étant donné un tenseur d'entrée `x` de forme [minibatch, ... , dim],\n",
    "         la couche d'intégration harmonique convertit chaque fonctionnalité\n",
    "         dans `x` dans une série de caractéristiques harmoniques `embedding`\n",
    "         comme suit:\n",
    "            embedding[..., i*dim:(i+1)*dim] = [\n",
    "                sin(x[..., i]),\n",
    "                sin(2*x[..., i]),\n",
    "                sin(4*x[..., i]),\n",
    "                ...\n",
    "                sin(2**(self.n_harmonic_functions-1) * x[..., i]),\n",
    "                cos(x[..., i]),\n",
    "                cos(2*x[..., i]),\n",
    "                cos(4*x[..., i]),\n",
    "                ...\n",
    "                cos(2**(self.n_harmonic_functions-1) * x[..., i])\n",
    "            ]\n",
    "\n",
    "        Notez que `x` est également prémultiplié par `omega0` avant\n",
    "         évaluer les fonctions harmoniques.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            'frequencies',\n",
    "            omega0 * (2.0 ** torch.arange(n_harmonic_functions)),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor of shape [..., dim]\n",
    "        Returns:\n",
    "            embedding: a harmonic embedding of `x`\n",
    "                of shape [..., n_harmonic_functions * dim * 2]\n",
    "        \"\"\"\n",
    "        embed = (x[..., None] * self.frequencies).view(*x.shape[:-1], -1)\n",
    "        return torch.cat((embed.sin(), embed.cos()), dim=-1)\n",
    "\n",
    "\n",
    "class NeuralRadianceField(torch.nn.Module):\n",
    "    def __init__(self, n_harmonic_functions=60, n_hidden_neurons=256):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "         Args :\n",
    "             n_harmonic_functions : Le nombre de fonctions harmoniques\n",
    "                 utilisé pour former l’intégration harmonique de chaque point.\n",
    "             n_hidden_neurons : le nombre d'unités cachées dans le\n",
    "                 couches entièrement connectées des MLP du modèle.\n",
    "         \"\"\"\n",
    "\n",
    "         # La couche d'intégration harmonique convertit les coordonnées 3D d'entrée\n",
    "         # à une représentation plus adaptée à\n",
    "         # traitement avec un réseau neuronal profond.\n",
    "        self.harmonic_embedding = HarmonicEmbedding(n_harmonic_functions)\n",
    "\n",
    "        # La dimension de l'intégration harmonique.\n",
    "        embedding_dim = n_harmonic_functions * 2 * 3\n",
    "\n",
    "        # self.mlp est un simple perceptron multicouche à 2 couches\n",
    "         # qui convertit les intégrations harmoniques d'entrée par point\n",
    "         # à une représentation latente.\n",
    "         # Non pas que nous utilisions les activations Softplus au lieu de ReLU.\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "        )\n",
    "\n",
    "        # Fonctionnalités données prédites par self.mlp, self.color_layer\n",
    "         # est chargé de prédire un vecteur 3D par point\n",
    "         # qui représente la couleur RVB du point.\n",
    "        self.color_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons + embedding_dim, n_hidden_neurons),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            torch.nn.Linear(n_hidden_neurons, 3),\n",
    "            torch.nn.Sigmoid(),\n",
    "            # Pour garantir que les couleurs se situent correctement entre [0-1],\n",
    "             # la couche se termine par une couche sigmoïde.\n",
    "        )\n",
    "\n",
    "        # La couche de densité convertit les caractéristiques de self.mlp\n",
    "         # à une valeur de densité 1D représentant l'opacité brute\n",
    "         # de chaque point.\n",
    "        self.density_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_hidden_neurons, 1),\n",
    "            torch.nn.Softplus(beta=10.0),\n",
    "            # L'activation de Sofplus garantit que l'opacité brute\n",
    "             # est un nombre non négatif.\n",
    "        )\n",
    "\n",
    "        # Nous fixons le biais de la couche de densité à -1,5\n",
    "         # afin d'initialiser les opacités des\n",
    "         # ray pointe vers des valeurs proches de 0.\n",
    "         # C'est un détail crucial pour assurer la convergence\n",
    "         # du modèle.\n",
    "        self.density_layer[0].bias.data[0] = -1.5\n",
    "\n",
    "    def _get_densities(self, features):\n",
    "        \"\"\"\n",
    "        Cette fonction prend les « fonctionnalités » prédites par « self.mlp »\n",
    "         et les convertit en `raw_densities` avec `self.density_layer`.\n",
    "         Les « raw_densities » sont ensuite mappées sur la plage [0-1] avec\n",
    "         1 - exponentielle inverse de `raw_densities`.\n",
    "        \"\"\"\n",
    "        raw_densities = self.density_layer(features)\n",
    "        return 1 - (-raw_densities).exp()\n",
    "\n",
    "    def _get_colors(self, features, rays_directions):\n",
    "        \"\"\"\n",
    "        Cette fonction prend les « fonctionnalités » par point prédites par « self.mlp »\n",
    "         et évalue le modèle de couleur afin de l'attacher à chacun\n",
    "         pointez un vecteur 3D de sa couleur RVB.\n",
    "\n",
    "         Afin de représenter les effets dépendants du point de vue,\n",
    "         avant d'évaluer `self.color_layer`, `NeuralRadianceField`\n",
    "         concatène aux « caractéristiques » un intégration harmonique\n",
    "         de `ray_directions`, qui sont des directions par point\n",
    "         de rayons ponctuels exprimés sous forme de vecteurs 3D normalisés l2\n",
    "         en coordonnées mondiales.\n",
    "        \"\"\"\n",
    "        spatial_size = features.shape[:-1]\n",
    "\n",
    "        # Normalisez les ray_directions à la norme de l'unité l2.\n",
    "        rays_directions_normed = torch.nn.functional.normalize(\n",
    "            rays_directions, dim=-1\n",
    "        )\n",
    "\n",
    "        # Obtenez l'intégration harmonique des directions des rayons normalisées.\n",
    "        rays_embedding = self.harmonic_embedding(\n",
    "            rays_directions_normed\n",
    "        )\n",
    "\n",
    "        # Développez le tenseur des directions des rayons afin que sa taille spatiale\n",
    "         # est égal à la taille des fonctionnalités.\n",
    "        rays_embedding_expand = rays_embedding[..., None, :].expand(\n",
    "            *spatial_size, rays_embedding.shape[-1]\n",
    "        )\n",
    "\n",
    "        # Concaténer les intégrations de direction de rayon avec\n",
    "         # fonctionnalités et évaluer le modèle de couleur.\n",
    "        color_layer_input = torch.cat(\n",
    "            (features, rays_embedding_expand),\n",
    "            dim=-1\n",
    "        )\n",
    "        return self.color_layer(color_layer_input)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        ray_bundle: RayBundle,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        La fonction forward accepte les paramétrages de\n",
    "         Points 3D échantillonnés le long des rayons de projection. L'avant\n",
    "         pass est responsable de l'attachement d'un vecteur 3D\n",
    "         et un scalaire 1D représentant le point\n",
    "         Couleur RVB et opacité respectivement.\n",
    "\n",
    "         Args :\n",
    "             ray_bundle : un objet RayBundle contenant les variables suivantes :\n",
    "                 origines : Un tenseur de forme `(minibatch, ..., 3)` désignant le\n",
    "                     origines des rayons d'échantillonnage dans les coordonnées mondiales.\n",
    "                 directions : Un tenseur de forme `(minibatch, ..., 3)`\n",
    "                     contenant les vecteurs de direction des rayons d'échantillonnage en coordonnées mondiales.\n",
    "                 lengths : Un tenseur de forme `(minibatch, ..., num_points_per_ray)`\n",
    "                     contenant les longueurs auxquelles les rayons sont échantillonnés.\n",
    "\n",
    "        Returns:\n",
    "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                denoting the opacity of each ray point.\n",
    "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
    "                denoting the color of each ray point.\n",
    "        \"\"\"\n",
    "        # Nous convertissons d'abord les paramètres des rayons en coordonnées mondiales avec `ray_bundle_to_ray_points`.\n",
    "        # coordonnées mondiales avec `ray_bundle_to_ray_points`.\n",
    "        rays_points_world = ray_bundle_to_ray_points(ray_bundle)\n",
    "        # rays_points_world.shape = [minibatch x ... x 3]\n",
    "\n",
    "        # Pour chaque coordonnée 3D du monde, nous obtenons son intégration harmonique.\n",
    "        embeds = self.harmonic_embedding(\n",
    "            rays_points_world\n",
    "        )\n",
    "        # embeds.shape = [minibatch x ... x self.n_harmonic_functions*6]\n",
    "\n",
    "        # self.mlp fait correspondre chaque intégration harmonique à un espace de caractéristiques latentes.\n",
    "        features = self.mlp(embeds)\n",
    "        # features.shape = [minibatch x ... x n_hidden_neurons]\n",
    "\n",
    "        # Enfin, étant donné les caractéristiques par point,\n",
    "        # exécuter les branches densité et couleur.\n",
    "\n",
    "        rays_densities = self._get_densities(features)\n",
    "        # rays_densities.shape = [minibatch x ... x 1]\n",
    "\n",
    "        rays_colors = self._get_colors(features, ray_bundle.directions)\n",
    "        # rays_colors.shape = [minibatch x ... x 3]\n",
    "\n",
    "        return rays_densities, rays_colors\n",
    "\n",
    "    def batched_forward(\n",
    "        self,\n",
    "        ray_bundle: RayBundle,\n",
    "        n_batches: int = 16,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Cette fonction est utilisée pour permettre un traitement efficace de la mémoire\n",
    "        des rayons d'entrée. Les rayons d'entrée sont d'abord divisés en morceaux `n_batches`\n",
    "        et sont passés à travers la fonction `self.forward` un par un\n",
    "        dans une boucle for. Combiné avec la désactivation de la mise en cache du gradient de PyTorch\n",
    "        (`torch.no_grad()`), cela permet de rendre de grands lots de\n",
    "        de rayons qui ne tiennent pas tous dans la mémoire du GPU en une seule passe.\n",
    "        Dans notre cas, batched_forward est utilisé pour exporter un rendu complet du champ de radiance pour la visualisation.\n",
    "        du champ de radiance à des fins de visualisation.\n",
    "\n",
    "        Args:\n",
    "            ray_bundle : Un objet RayBundle contenant les variables suivantes :\n",
    "                origines : Un tenseur de la forme `(minibatch, ..., 3)` représentant les\n",
    "                    origines des rayons d'échantillonnage en coordonnées mondiales.\n",
    "                directions : Un tenseur de forme `(minibatch, ..., 3)`\n",
    "                    contenant les vecteurs de direction des rayons d'échantillonnage en coordonnées mondiales.\n",
    "                lengths : Un tenseur de forme `(minibatch, ..., num_points_per_ray)`\n",
    "                    contenant les longueurs auxquelles les rayons sont échantillonnés.\n",
    "            n_batches : Spécifie le nombre de lots dans lesquels les rayons d'entrée sont divisés.\n",
    "                Plus le nombre de lots est élevé, plus l'empreinte mémoire est faible et plus la vitesse de traitement est réduite.\n",
    "                et plus la vitesse de traitement est faible.\n",
    "\n",
    "        Returns:\n",
    "            rays_densities : Un tenseur de la forme `(minibatch, ..., num_points_per_ray, 1)`\n",
    "                indiquant l'opacité de chaque point du rayon.\n",
    "            rays_colors : Un tenseur de forme `(minibatch, ..., num_points_par_ray, 3)`\n",
    "                indiquant la couleur de chaque point de rayon.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #Cette fonction analyse les formes nécessaires au remodelage du tenseur.\n",
    "        n_pts_per_ray = ray_bundle.lengths.shape[-1]\n",
    "        spatial_size = [*ray_bundle.origins.shape[:-1], n_pts_per_ray]\n",
    "\n",
    "        # Divisez les rayons en lots `n_batches`.\n",
    "        tot_samples = ray_bundle.origins.shape[:-1].numel()\n",
    "        batches = torch.chunk(torch.arange(tot_samples), n_batches)\n",
    "\n",
    "        # Pour chaque lot, exécutez la passe avant standard.\n",
    "        batch_outputs = [\n",
    "            self.forward(\n",
    "                RayBundle(\n",
    "                    origins=ray_bundle.origins.view(-1, 3)[batch_idx],\n",
    "                    directions=ray_bundle.directions.view(-1, 3)[batch_idx],\n",
    "                    lengths=ray_bundle.lengths.view(-1, n_pts_per_ray)[batch_idx],\n",
    "                    xys=None,\n",
    "                )\n",
    "            ) for batch_idx in batches\n",
    "        ]\n",
    "\n",
    "        # Concaténer les rayons_densités et les rayons_colors par lot\n",
    "         # et remodeler en fonction des tailles des entrées.\n",
    "        rays_densities, rays_colors = [\n",
    "            torch.cat(\n",
    "                [batch_output[output_i] for batch_output in batch_outputs], dim=0\n",
    "            ).view(*spatial_size, -1) for output_i in (0, 1)\n",
    "        ]\n",
    "        return rays_densities, rays_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua0f-z6o_gH3"
   },
   "source": [
    "## 4. Fonctions d'assistance\n",
    "\n",
    "Dans cette fonction, nous définissons des fonctions qui aident à l'optimisation du champ de radiance neuronale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1701780483327,
     "user": {
      "displayName": "nabil bcom",
      "userId": "04169804032469069549"
     },
     "user_tz": -60
    },
    "id": "7yuE-FTz_gH4"
   },
   "outputs": [],
   "source": [
    "def huber(x, y, scaling=0.1):\n",
    "    \"\"\"\n",
    "     Une fonction d'assistance pour évaluer la perte douce de L1 (huber)\n",
    "     entre les silhouettes et les couleurs rendues.\n",
    "     \"\"\"\n",
    "    diff_sq = (x - y) ** 2\n",
    "    loss = ((1 + diff_sq / (scaling**2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n",
    "    return loss\n",
    "\n",
    "def sample_images_at_mc_locs(target_images, sampled_rays_xy):\n",
    "    \"\"\"\n",
    "     Étant donné un ensemble d'emplacements de pixels de Monte Carlo `sampled_rays_xy`,\n",
    "     cette méthode échantillonne le tenseur `target_images` au\n",
    "     emplacements 2D respectifs.\n",
    "\n",
    "     Cette fonction est utilisée afin d'extraire les couleurs de\n",
    "     des images de vérité terrain qui correspondent aux couleurs\n",
    "     rendu à l'aide de `MonteCarloRaysampler`.\n",
    "     \"\"\"\n",
    "    ba = target_images.shape[0]\n",
    "    dim = target_images.shape[-1]\n",
    "    spatial_size = sampled_rays_xy.shape[1:-1]\n",
    "    # Afin d'échantillonner target_images, nous utilisons\n",
    "     # la fonction grid_sample qui implémente un\n",
    "     # échantillonneur d'images bilinéaires.\n",
    "     # Notez qu'il faut inverser le signe du\n",
    "     # positions de rayons échantillonnées pour convertir les emplacements NDC xy\n",
    "     # du MonteCarloRaysampler à la coordonnée\n",
    "     # convention de grid_sample.\n",
    "    images_sampled = torch.nn.functional.grid_sample(\n",
    "        target_images.permute(0, 3, 1, 2),\n",
    "        -sampled_rays_xy.view(ba, -1, 1, 2),  # note the sign inversion\n",
    "        align_corners=True\n",
    "    )\n",
    "    return images_sampled.permute(0, 2, 3, 1).view(\n",
    "        ba, *spatial_size, dim\n",
    "    )\n",
    "\n",
    "def show_full_render(\n",
    "    neural_radiance_field, camera,\n",
    "    target_image, target_silhouette,\n",
    "    loss_history_color, loss_history_sil,\n",
    "):\n",
    "    \"\"\"\n",
    "    Il s'agit d'une fonction d'assistance pour visualiser le\n",
    "     résultats intermédiaires de l’apprentissage.\n",
    "\n",
    "     Puisque le `NeuralRadianceField` souffre de\n",
    "     une empreinte mémoire importante, qui ne nous permet pas\n",
    "     rendre la grille d'image complète en un seul passage,\n",
    "     nous utilisons le `NeuralRadianceField.batched_forward`\n",
    "     fonction en combinaison avec la désactivation de la mise en cache du dégradé.\n",
    "     Cela divise l'ensemble des rayons émis en lots et\n",
    "     évalue la fonction implicite sur un lot à la fois\n",
    "     pour éviter le débordement de la mémoire GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # Empêcher la mise en cache des dégradés.\n",
    "    with torch.no_grad():\n",
    "        # Rendu en utilisant le moteur de rendu de grille et le\n",
    "         # Fonction batched_forward de neural_radiance_field.\n",
    "        rendered_image_silhouette, _ = renderer_grid(\n",
    "            cameras=camera,\n",
    "            volumetric_function=neural_radiance_field.batched_forward\n",
    "        )\n",
    "        # Diviser le résultat du rendu en un rendu de silhouette\n",
    "         # et le rendu de l'image.\n",
    "        rendered_image, rendered_silhouette = (\n",
    "            rendered_image_silhouette[0].split([3, 1], dim=-1)\n",
    "        )\n",
    "\n",
    "    # Générer des tracés.\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    ax = ax.ravel()\n",
    "    clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()\n",
    "    ax[0].plot(list(range(len(loss_history_color))), loss_history_color, linewidth=1)\n",
    "    ax[1].imshow(clamp_and_detach(rendered_image))\n",
    "    ax[2].imshow(clamp_and_detach(rendered_silhouette[..., 0]))\n",
    "    ax[3].plot(list(range(len(loss_history_sil))), loss_history_sil, linewidth=1)\n",
    "    ax[4].imshow(clamp_and_detach(target_image))\n",
    "    ax[5].imshow(clamp_and_detach(target_silhouette))\n",
    "    for ax_, title_ in zip(\n",
    "        ax,\n",
    "        (\n",
    "            \"loss color\", \"rendered image\", \"rendered silhouette\",\n",
    "            \"loss silhouette\", \"target image\",  \"target silhouette\",\n",
    "        )\n",
    "    ):\n",
    "        if not title_.startswith('loss'):\n",
    "            ax_.grid(\"off\")\n",
    "            ax_.axis(\"off\")\n",
    "        ax_.set_title(title_)\n",
    "    fig.canvas.draw(); fig.show()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zee9Bp3Q_gH6"
   },
   "source": [
    "## 5. Ajuster le champ de radiance\n",
    "\n",
    "Nous réalisons ici l'ajustement du champ de radiance avec un rendu différentiable.\n",
    "\n",
    "Afin d'ajuster le champ de radiance, nous le rendons du point de vue des `target_cameras`\n",
    "et comparez les rendus résultants avec les «target_images» et «target_silhouettes» observées.\n",
    "\n",
    "La comparaison est effectuée en évaluant l'erreur moyenne de Huber (smooth-l1) entre les\n",
    "paires de `target_images`/`rendered_images` et `target_silhouettes`/`rendered_silhouettes`.\n",
    "\n",
    "Puisque nous utilisons le `MonteCarloRaysampler`, les sorties du moteur de rendu de formation `renderer_mc`\n",
    "sont des couleurs de pixels échantillonnés aléatoirement dans le plan de l'image, et non un réseau de pixels formant\n",
    "une image valide. Ainsi, afin de comparer les couleurs rendues avec la vérité terrain, nous\n",
    "utiliser les emplacements aléatoires des pixels de MonteCarlo pour échantillonner les images/silhouettes de vérité terrain\n",
    "`target_silhouettes`/`rendered_silhouettes` aux emplacements xy correspondant au rendu\n",
    "Emplacements. Cela se fait avec la fonction d'assistance `sample_images_at_mc_locs`, qui est\n",
    "décrit dans la cellule précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 996,
     "output_embedded_package_id": "1HHm2MagVZeXzViOuS93K9dTiV2nx088F"
    },
    "id": "qyFZlCbz_gH7",
    "outputId": "56fb5dd6-70eb-4701-a484-60e873d8dde5"
   },
   "outputs": [],
   "source": [
    "# Déplacez d'abord toutes les variables pertinentes vers le bon appareil.\n",
    "renderer_grid = renderer_grid.to(device)\n",
    "renderer_mc = renderer_mc.to(device)\n",
    "target_cameras = target_cameras.to(device)\n",
    "target_images = target_images.to(device)\n",
    "target_silhouettes = target_silhouettes.to(device)\n",
    "\n",
    "# Définir la graine pour la reproductibilité\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Instanciez le modèle de champ de radiance.\n",
    "neural_radiance_field = NeuralRadianceField().to(device)\n",
    "\n",
    "# Instanciez l'optimiseur Adam. Nous avons fixé son taux d'apprentissage principal à 1e-3.\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=lr)\n",
    "\n",
    "# Nous échantillonnons 6 caméras aléatoires dans un mini-lot. Chaque caméra\n",
    "# émet des rayonsrayampler_mc.n_pts_per_image.\n",
    "batch_size = 6\n",
    "\n",
    "# 3000 itérations prennent environ 20 minutes sur une Tesla M40 et conduisent à\n",
    "# des résultats raisonnablement nets. Cependant, pour le meilleur possible\n",
    "# résultats, nous vous recommandons de définir n_iter=20000.\n",
    "n_iter = 3000\n",
    "\n",
    "# Initialisez les tampons de l'historique des pertes.\n",
    "loss_history_color, loss_history_sil = [], []\n",
    "\n",
    "# La boucle d'optimisation principale.\n",
    "for iteration in range(n_iter):\n",
    "    # Dans le cas où nous aurions atteint les derniers 75 % d'itérations,\n",
    "     # diminuer le taux d'apprentissage de l'optimiseur de 10 fois.\n",
    "    if iteration == round(n_iter * 0.75):\n",
    "        print('Decreasing LR 10-fold ...')\n",
    "        optimizer = torch.optim.Adam(\n",
    "            neural_radiance_field.parameters(), lr=lr * 0.1\n",
    "        )\n",
    "\n",
    "    # Zéro le gradient de l'optimiseur.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Échantillon d'indices de lots aléatoires.\n",
    "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
    "\n",
    "    # Échantillonnez le minilot de caméras.\n",
    "    batch_cameras = FoVPerspectiveCameras(\n",
    "        R = target_cameras.R[batch_idx],\n",
    "        T = target_cameras.T[batch_idx],\n",
    "        znear = target_cameras.znear[batch_idx],\n",
    "        zfar = target_cameras.zfar[batch_idx],\n",
    "        aspect_ratio = target_cameras.aspect_ratio[batch_idx],\n",
    "        fov = target_cameras.fov[batch_idx],\n",
    "        device = device,\n",
    "    )\n",
    "\n",
    "    # Évaluez le modèle nerf.\n",
    "    rendered_images_silhouettes, sampled_rays = renderer_mc(\n",
    "        cameras=batch_cameras,\n",
    "        volumetric_function=neural_radiance_field\n",
    "    )\n",
    "    rendered_images, rendered_silhouettes = (\n",
    "        rendered_images_silhouettes.split([3, 1], dim=-1)\n",
    "    )\n",
    "\n",
    "    # Calculez l'erreur de silhouette comme le huber moyen\n",
    "     # perte entre les masques prédits et le\n",
    "     # silhouettes cibles échantillonnées.\n",
    "    silhouettes_at_rays = sample_images_at_mc_locs(\n",
    "        target_silhouettes[batch_idx, ..., None],\n",
    "        sampled_rays.xys\n",
    "    )\n",
    "    sil_err = huber(\n",
    "        rendered_silhouettes,\n",
    "        silhouettes_at_rays,\n",
    "    ).abs().mean()\n",
    "\n",
    "    # Calculez l'erreur de couleur comme le huber moyen\n",
    "     # perte entre les couleurs rendues et le\n",
    "     # images cibles échantillonnées.\n",
    "    colors_at_rays = sample_images_at_mc_locs(\n",
    "        target_images[batch_idx],\n",
    "        sampled_rays.xys\n",
    "    )\n",
    "    color_err = huber(\n",
    "        rendered_images,\n",
    "        colors_at_rays,\n",
    "    ).abs().mean()\n",
    "\n",
    "    # La perte d'optimisation est simple\n",
    "     # somme des erreurs de couleur et de silhouette.\n",
    "    loss = color_err + sil_err\n",
    "\n",
    "    # Enregistrez l'historique des pertes.\n",
    "    loss_history_color.append(float(color_err))\n",
    "    loss_history_sil.append(float(sil_err))\n",
    "\n",
    "    # Toutes les 10 itérations, imprimez les valeurs actuelles des pertes.\n",
    "    if iteration % 10 == 0:\n",
    "        print(\n",
    "            f'Iteration {iteration:05d}:'\n",
    "            + f' loss color = {float(color_err):1.2e}'\n",
    "            + f' loss silhouette = {float(sil_err):1.2e}'\n",
    "        )\n",
    "\n",
    "    # Passez à l'étape d'optimisation.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Visualisez les rendus complets toutes les 100 itérations.\n",
    "    if iteration % 100 == 0:\n",
    "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
    "        show_full_render(\n",
    "            neural_radiance_field,\n",
    "            FoVPerspectiveCameras(\n",
    "                R = target_cameras.R[show_idx],\n",
    "                T = target_cameras.T[show_idx],\n",
    "                znear = target_cameras.znear[show_idx],\n",
    "                zfar = target_cameras.zfar[show_idx],\n",
    "                aspect_ratio = target_cameras.aspect_ratio[show_idx],\n",
    "                fov = target_cameras.fov[show_idx],\n",
    "                device = device,\n",
    "            ),\n",
    "            target_images[show_idx][0],\n",
    "            target_silhouettes[show_idx][0],\n",
    "            loss_history_color,\n",
    "            loss_history_sil,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_i66vUnj_gH7"
   },
   "source": [
    "## 6. Visualiser le champ de rayonnement neuronal optimisé\n",
    "\n",
    "Enfin, nous visualisons le champ de radiance neuronale en effectuant un rendu à partir de plusieurs points de vue qui tournent autour de l'axe y du volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsuqOXyY_gH8"
   },
   "outputs": [],
   "source": [
    "def generate_rotating_nerf(neural_radiance_field, n_frames = 50):\n",
    "    logRs = torch.zeros(n_frames, 3, device=device)\n",
    "    logRs[:, 1] = torch.linspace(-3.14, 3.14, n_frames, device=device)\n",
    "    Rs = so3_exp_map(logRs)\n",
    "    Ts = torch.zeros(n_frames, 3, device=device)\n",
    "    Ts[:, 2] = 2.7\n",
    "    frames = []\n",
    "    print('Rendering rotating NeRF ...')\n",
    "    for R, T in zip(tqdm(Rs), Ts):\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R[None],\n",
    "            T=T[None],\n",
    "            znear=target_cameras.znear[0],\n",
    "            zfar=target_cameras.zfar[0],\n",
    "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
    "            fov=target_cameras.fov[0],\n",
    "            device=device,\n",
    "        )\n",
    "        # Notez que nous rendons à nouveau avec `NDCMultinomialRaysampler`\n",
    "         # et la fonction batched_forward de neural_radiance_field.\n",
    "        frames.append(\n",
    "            renderer_grid(\n",
    "                cameras=camera,\n",
    "                volumetric_function=neural_radiance_field.batched_forward,\n",
    "            )[0][..., :3]\n",
    "        )\n",
    "    return torch.cat(frames)\n",
    "\n",
    "with torch.no_grad():\n",
    "    rotating_nerf_frames = generate_rotating_nerf(neural_radiance_field, n_frames=3*5)\n",
    "\n",
    "image_grid(rotating_nerf_frames.clamp(0., 1.).cpu().numpy(), rows=3, cols=5, rgb=True, fill=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
